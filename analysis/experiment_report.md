# Turing Game Experiment Results

## Overall Model Performance

- **gemini-1.5-pro**: 36.7% (22/60)

## Performance by File


### gemini-1.5-pro

- **Day_1_Ivy.md**: 30.0% accuracy
- **Day_1_Hal.md**: 60.0% accuracy
- **Day_1_Gus.md**: 40.0% accuracy
- **Day_1_Kim.md**: 10.0% accuracy
- **Day_1_Jan.md**: 50.0% accuracy
- **Day_1_Leo.md**: 30.0% accuracy

## Most Challenging Games

The following games were the most difficult for models to correctly identify the bot:
- Game 1003
- Game 1902
- Game 1503
- Game 1501
- Game 1101

## Easiest Games

The following games were the easiest for models to correctly identify the bot:
- Game 1401
- Game 1603
- Game 1402
- Game 1102
- Game 1403

## Conclusion

The models struggled to reliably detect bots beyond random chance (33.33%).